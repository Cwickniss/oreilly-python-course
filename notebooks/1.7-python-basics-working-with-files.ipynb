{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Files\n",
    "\n",
    "Lesson 11 - txt files python , open read, store contents to file, extract information with LLMs in bullet points to txt file, structure prompts to extract desired data from txt file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python it is extremely simple to work with files like .txt or .md files for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_tools import ask_ai\n",
    "from IPython.display import Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://arxiv.org/pdf/2412.14161\n",
      "\n",
      "The paper titled \"TheAgent Company: Benchmarking LLM Agents on Consequential Real World Tasks\" introduces a new benchmark, TheAgentCompany, aimed at evaluating the efficacy of large language model (LLM)-powered AI agents in completing real-world professional tasks in a simulated software company environment. The research is conducted by a collaborative team, mainly from Carnegie Mellon University and other institutions, and emphasizes the growing presence of AI in work settings.\n",
      "\n",
      "### Key Points from the Paper:\n",
      "\n",
      "1. **Motivation**:\n",
      "   - The rapid advancements in LLMs are prompting questions about AI's potential to automate or assist in various work-related tasks.\n",
      "   - Understanding AI agents’ capabilities is crucial for businesses considering AI integration and for policymakers assessing AI’s impact on employment.\n",
      "\n",
      "2. **Benchmark Overview**:\n",
      "   - TheAgentCompany simulates a software company environment with 175 diverse professional tasks spanning categories like software engineering, project management, and finance.\n",
      "   - The benchmark allows agents to interact through web browsing, coding, and colleague communication, providing a realistic testing framework.\n",
      "\n",
      "3. **Performance Findings**:\n",
      "   - Experiments conducted with several LLMs, including closed (like OpenAI's GPT-4o and Claude) and open-weight models (like Llama), reveal that the top-performing model, Claude-3.5-Sonnet, achieved 24% task completion autonomously, with a score of 34.4% when accounting for partial completions.\n",
      "   - Despite these advancements, LLM agents struggle significantly with longer, more complex tasks, especially those requiring social interaction and navigation of intricate user interfaces.\n",
      "\n",
      "4. **Framework and Design**:\n",
      "   - TheAgentCompany provides a self-hosted and reproducible environment utilizing open-source software.\n",
      "   - Tasks are structured into parts with defined checkpoints, allowing agents to receive partial credit for incomplete tasks.\n",
      "   - Evaluators for tasks are tailored to assess not just the success of task completion but also the quality of interactions with simulated colleagues.\n",
      "\n",
      "5. **Interaction and Collaboration**:\n",
      "   - A significant component of the benchmark involves the ability of agents to communicate effectively with simulated colleagues within the environment, enhancing the realism and complexity of tasks.\n",
      "\n",
      "6. **Future Directions**:\n",
      "   - The paper suggests that while TheAgentCompany provides a foundational step for understanding LLM capabilities in professional settings, there is a need to expand the tasks covered and include more creative or less straightforward tasks.\n",
      "   - Continuous improvements in LLMs are expected, highlighting their potential for increased efficiency and performance across various domains.\n",
      "\n",
      "7. **Conclusions**:\n",
      "   - The research underscores the current limitations of LLM agents in effectively automating diverse professional tasks.\n",
      "   - The results serve as a litmus test for future developments, pointing towards areas where LLM technology must improve, particularly in tasks involving human-like social interactions and complex decision-making.\n",
      "\n",
      "In summary, TheAgentCompany represents a significant effort to quantify AI agents' performance in real-world applications and to chart a course for further research in this rapidly evolving field.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To open a file\n",
    "\n",
    "with open(\"./file.txt\", \"r\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular file we read a summary of a paper called: [\"TheAgentCompany: Benchmarking LLM Agents on Clnsequential Real World Tasks\"](https://arxiv.org/pdf/2412.14161)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also create files easily in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "content = \"This is a file\"\n",
    "with open(\"summary-notes.txt\", \"w\") as f:\n",
    "    f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a file"
     ]
    }
   ],
   "source": [
    "# in the cmd below we print the contents of an existing file in the current directory\n",
    "!cat ./summary-notes.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are more examples for the different modes of reading and writing files available via the built-in `open()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common file modes in Python's open() function:\n",
    "\n",
    "# \"a\" - Append - Opens file for appending, creates new file if not exists\n",
    "with open(\"file.txt\", \"a\") as f:\n",
    "    f.write(\"append this\")\n",
    "\n",
    "# \"x\" - Exclusive creation - Opens for writing, fails if file exists\n",
    "try:\n",
    "    with open(\"newfile.txt\", \"x\") as f:\n",
    "        f.write(\"new file content\")\n",
    "except FileExistsError:\n",
    "    print(\"File already exists\")\n",
    "\n",
    "# \"+\" - Read and write mode\n",
    "with open(\"file.txt\", \"r+\") as f:  # Open for both reading and writing\n",
    "    data = f.read()\n",
    "    f.write(\"new data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cool stuff about being able to do this is that we can connect our ability of generating summaries of information with AI, along with our ability to read and write files in Python to create super powerful workflows.\n",
    "\n",
    "For example, below we will write single sentence summaries for multiple files containing information about different papers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Paper Summaries\n",
       "\n",
       "## Paper 1\n",
       "\n",
       "The paper \"TheAgent Company: Benchmarking LLM Agents on Consequential Real World Tasks\" introduces TheAgentCompany, a benchmark designed to evaluate large language model (LLM) agents in a simulated software company environment. It highlights the potential and limitations of LLMs in automating professional tasks, revealing that while models like Claude-3.5-Sonnet show promise with a 24% autonomous task completion rate, they struggle with complex tasks requiring social interaction. The benchmark provides a reproducible framework for assessing AI capabilities and suggests future research directions to enhance LLM performance in professional settings.\n",
       "\n",
       "## Paper 2\n",
       "\n",
       "The paper \"Automated Design of Agentic Systems\" introduces a new research area called Automated Design of Agentic Systems (ADAS), which aims to automatically create powerful agentic systems by inventing novel building blocks and combining them in innovative ways. The authors propose an approach where agents are defined in code, allowing a meta agent to iteratively discover and program new agents. They present an algorithm called Meta Agent Search, which demonstrates the potential of this approach by progressively inventing agents that outperform state-of-the-art hand-designed agents across various domains. The paper highlights the robustness and generality of the discovered agents, emphasizing the potential of ADAS to automate the design of agentic systems and benefit humanity.\n",
       "\n",
       "## Paper 3\n",
       "\n",
       "The paper \"Who Validates the Validators? Aligning LLM-Assisted Evaluation of LLM Outputs with Human Preferences\" introduces EvalGen, a tool designed to help align evaluations of large language model (LLM) outputs with human preferences. The authors highlight the challenges of relying on LLMs for evaluation due to their inherent biases and propose a mixed-initiative approach where humans provide feedback to refine evaluation criteria and assertions. EvalGen assists users by generating evaluation criteria and candidate implementations, which are then aligned with human feedback through a grading process. The study reveals the phenomenon of \"criteria drift,\" where users refine their evaluation criteria as they grade more outputs, suggesting the need for iterative and flexible evaluation processes. The paper concludes with implications for designing future LLM evaluation assistants, emphasizing the importance of supporting iterative refinement and alignment with user preferences.\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_with_papers = \"./assets-resources/papers/\"\n",
    "file_names = [\"paper1.txt\", \"paper2.txt\", \"paper3.txt\"]\n",
    "\n",
    "def summarize_this_paper(paper_contents):\n",
    "    summary_prompt = f\"Summarize this paper\\n\\n: {paper_contents} in a couple of sentences.\"\n",
    "    output_summary = ask_ai(summary_prompt)\n",
    "    \n",
    "    return output_summary\n",
    "\n",
    "paper_summaries_list = []\n",
    "for file_name in file_names:\n",
    "    file_path = folder_with_papers + file_name\n",
    "    with open(file_path, \"r\") as f:\n",
    "        contents_of_the_paper = f.read()\n",
    "    \n",
    "    paper_summary = summarize_this_paper(contents_of_the_paper)\n",
    "    paper_summaries_list.append(paper_summary)\n",
    "            \n",
    "\n",
    "# Display the markdown content in the notebook\n",
    "    \n",
    "markdown_content = \"# Paper Summaries\\n\\n\"\n",
    "for i, summary in enumerate(paper_summaries_list, 1):\n",
    "    markdown_content += f\"## Paper {i}\\n\\n\"\n",
    "    markdown_content += f\"{summary}\\n\\n\"\n",
    "        \n",
    "Markdown(markdown_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do similar things to extract specific information from documents, imagine you have a bunch of differently formatted invoices from which you would like to organize the information extracting things like the amounts and dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2024-04-18 - $48,727.50'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_invoice_data(invoice_contents):\n",
    "    extraction_prompt = f\"\"\"Extract the following information from this invoice:\n",
    "    - Date\n",
    "    - Total Amount\n",
    "    \n",
    "    Invoice contents:\n",
    "    {invoice_contents}\n",
    "    \n",
    "    Return only the date and total amount separated by dashes\n",
    "    like this:\n",
    "    \n",
    "    2024-04-15 - $18,900.00\n",
    "    2024-04-18 - $48,727.50 \n",
    "    2023-12-31 - €2,350.44\n",
    "    \n",
    "    Extracted information:\n",
    "    \"\"\"\n",
    "    \n",
    "    extracted_data = ask_ai(extraction_prompt)\n",
    "    return extracted_data\n",
    "\n",
    "example_data = \"\"\"\n",
    "TECH SOLUTIONS INC.\n",
    "789 Innovation Drive\n",
    "Seattle, WA 98101\n",
    "Tax ID: 98-7654321\n",
    "\n",
    "INVOICE\n",
    "\n",
    "Bill To:                                    Invoice No: INV-2024-0103 \n",
    "Sarah Johnson                               Date: April 18, 2024\n",
    "789 Enterprise Road                         Due Date: May 18, 2024\n",
    "Chicago, IL 60601\n",
    "\n",
    "Description                     Quantity    Rate        Amount\n",
    "-----------------------------------------------------------------\n",
    "AI Model Development              120      $200.00    $24,000.00\n",
    "Data Processing Services           80      $125.00    $10,000.00\n",
    "System Integration                 40      $175.00     $7,000.00\n",
    "Hardware Configuration             1     $3,500.00     $3,500.00\n",
    "                                                    ------------\n",
    "                                           Subtotal:  $44,500.00\n",
    "                                           Tax (9.5%): $4,227.50\n",
    "                                           Total:     $48,727.50\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "extract_invoice_data(example_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply this to all the invoice data we have!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'file': 'invoice1.txt', 'date': '2023-12-31', 'amount': '€2,350.44'},\n",
       " {'file': 'invoice2.txt', 'date': '2024-04-15', 'amount': '$18,900.00'},\n",
       " {'file': 'invoice3.txt', 'date': '2024-04-18', 'amount': '$48,727.50'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folder_with_invoices = \"./assets-resources/fake-invoices/\"\n",
    "invoice_files = [\"invoice1.txt\", \"invoice2.txt\", \"invoice3.txt\"]\n",
    "\n",
    "invoice_data_list = []\n",
    "for invoice_file in invoice_files:\n",
    "    file_path = folder_with_invoices + invoice_file\n",
    "    with open(file_path, \"r\") as f:\n",
    "        invoice_contents = f.read()\n",
    "    \n",
    "    extracted_data = extract_invoice_data(invoice_contents)\n",
    "    date, amount = extracted_data.split(\" - \")\n",
    "    invoice_data = {\n",
    "        \"file\": invoice_file,\n",
    "        \"date\": date,\n",
    "        \"amount\": amount\n",
    "    }\n",
    "    \n",
    "    invoice_data_list.append(invoice_data)\n",
    "\n",
    "invoice_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Invoice Data Summary\n",
       "\n",
       "## invoice1.txt\n",
       "\n",
       "**Date:** 2023-12-31\n",
       "\n",
       "**Amount:** €2,350.44\n",
       "\n",
       "## invoice2.txt\n",
       "\n",
       "**Date:** 2024-04-15\n",
       "\n",
       "**Amount:** $18,900.00\n",
       "\n",
       "## invoice3.txt\n",
       "\n",
       "**Date:** 2024-04-18\n",
       "\n",
       "**Amount:** $48,727.50\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the markdown content in the notebook\n",
    "markdown_content = \"# Invoice Data Summary\\n\\n\"\n",
    "for invoice in invoice_data_list:\n",
    "    markdown_content += f\"## {invoice['file']}\\n\\n\"\n",
    "    markdown_content += f\"**Date:** {invoice['date']}\\n\\n\"\n",
    "    markdown_content += f\"**Amount:** {invoice['amount']}\\n\\n\"\n",
    "\n",
    "Markdown(markdown_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "oreilly-automate-tasks",
   "language": "python",
   "name": "oreilly-automate-tasks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
